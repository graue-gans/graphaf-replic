import torchimport torch.nn as nnimport torch.nn.functional as Fclass MLP(nn.Module):    def __init__(self, input_size, hidden_size, output_size):        super(MLP, self).__init__()        self.fc1 = nn.Linear(input_size, hidden_size)        self.fc2 = nn.Linear(hidden_size, output_size)        self.tanh = nn.Tanh()    def forward(self, x):        x = self.fc1(x)        x = self.tanh(x)        x = self.fc2(x)        x = self.tanh(x)        return xclass RGCN(nn.Module):    def __init__(self, embedding_dim, b, L=3, agg=torch.sum):        super(RGCN, self).__init__()        self.embedding_dim = embedding_dim        self.b = b        self.L = L        self.agg = agg        self.weights = nn.ParameterList(            [nn.Parameter(torch.randn(self.embedding_dim, self.embedding_dim, self.b)) for i in range(self.L)]        )    def forward(self, graph):        self.X, self.A = graph        self.N = self.X.shape[0]        H_prev = self.X        for l in range(self.L):            W = self.weights[l]            tensor_list = []            for i in range(self.b):  # FIXME convert to tensor factorization                E_i = self.A[:, :, i] + torch.eye(self.N)  # dim: n x n                D_i = torch.sum(E_i, dim=1)  # degree vector                D_inv_sqrt = torch.diag(torch.pow(D_i, -0.5))  # dim: n x n                z = F.ReLu(D_inv_sqrt @ E_i @ D_inv_sqrt @ H_prev @ W[:, :, i])  # dim: n x k                tensor_list.append(z)            Z = torch.stack(tensor_list, dim=2)  # dim: n x k x b            H = self.agg(Z, dim=2)  # dim: n x k            H_prev = H        return Hclass GraphAF(nn.Module):    def __init__(self):        super(GraphAF, self).__init__()        # dimensions        embedding_dim = 128        d = 8  # number of node types; FIXME dummy value        b = 3  # number of edge types; FIXME confirm value        # MLPs        # FIXME hidden dim not known        # FIXME edge mlp input dim not known        self.mu_node = MLP(embedding_dim, 2 * embedding_dim, d)        self.alpha_node = MLP(embedding_dim, 2 * embedding_dim, d)        self.mu_edge = MLP(3 * embedding_dim, 2 * 3 * embedding_dim, b + 1)        self.alpha_edge = MLP(3 * embedding_dim, 2 * 3 * embedding_dim, b + 1)        # R-GCN        self.rgcn = RGCN(embedding_dim, b)    def forward(self, graph):        pass    def generate(self, num_samples=1):        with torch.no_grad():            pass