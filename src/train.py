import torchfrom model import GraphAFfrom torch.distributions import MultivariateNormaldef main():    # Setup    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")    model = GraphAF().to(device)    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)    train_loader, val_loader = get_dataloaders()    num_epochs = 10    batch_size = 32    epsilon_node = MultivariateNormal(torch.zeros(d), torch.eye(d))    epsilon_edge = MultivariateNormal(torch.zeros(b + 1), torch.eye(b + 1))    P = 12    # Training loop    for epoch in range(num_epochs):  # while theta is not converged        # train_loss = train_one_epoch(model, train_loader, optimizer, device)        # torch.save(model.state_dict(), "checkpoint.pt")        loss = 0        optimizer.zero_grad()        for m in range(batch_size):  # this loops over data in one batch, conversion necessary            # sample molecule from dataset and get graph size N            # convert molecule to graph with BFS ordering            node_loss = 0            edge_loss = 0            for i in range(N):                # Message passing                H_i = model.rgcn().forward(subgraph)  # dim: n x k                h_i = torch.sum(H_i, dim=0)  # dim: k                #                z_i = X[i, :] + torch.rand(d)  # dim: d                mu_i = model.mu_node.forward(h_i)  # dim: d                alpha_i = model.alpha_node.forward(h_i)  # dim: d                epsilon_i = (z_i - mu_i) * (1 / alpha_i)  # dim: d                # - sum log p(epsilon_i) = - log prod p(epsilon_i)                node_loss += -torch.sum(epsilon_node.log_prob(epsilon_i)) - torch.log(torch.prod(1 / alpha_i))                for j in range(max(1, i - P), i - 1):                    z_ij = A[i, j, :] + torch.rand(b + 1)  # dim: b+1                    mu_ij = model.mu_edge.forward(h_i, H_i[i], H_i[j])  # dim: b+1                    alpha_ij = model.alpha_edge.forward(h_i, H_i[i], H_i[j])  # dim: b+1                    epsilon_ij = (z_ij - mu_ij) * (1 / alpha_ij)  # dim: b+1                    # - sum log p(epsilon_ij) = - log prod p(epsilon_ij)                    edge_loss += -torch.sum(epsilon_edge.log_prob(epsilon_ij)) - torch.log(torch.prod(1 / alpha_ij))            loss += node_loss + edge_loss        loss.backward()        optimizer.step()if __name__ == "__main__":    main()